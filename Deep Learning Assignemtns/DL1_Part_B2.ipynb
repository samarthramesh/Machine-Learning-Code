{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL1 Part B2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKsI8E6Qd4Ue"
      },
      "source": [
        "import torch, os\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2NvwLaVeFDf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "df9cd3c6-ca9b-4b91-a81d-a7179ac25323"
      },
      "source": [
        "########################################################################\n",
        "# The output of torchvision datasets are PILImage images of range [0, 1].\n",
        "\n",
        "# Apply necessary image transfromations here \n",
        "\n",
        "transform = transforms.Compose([#torchvision.transforms.RandomAffine(degrees=(-10, 10), translate=(0.1, 0.1), scale=(0.8, 1.2)),\n",
        "                                #torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
        "                                #torchvision.transforms.RandomHorizontalFlip(),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5, 0.5, 0.5])])\n",
        "print(transform)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compose(\n",
            "    ToTensor()\n",
            "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj4qBTV9eFq5"
      },
      "source": [
        "train_data_dir = '/content/drive/My Drive/Datasets/3/train' # put path of training dataset\n",
        "val_data_dir = '/content/drive/My Drive/Datasets/3/val' # put path of validation dataset\n",
        "test_data_dir = '/content/drive/My Drive/Datasets/3/test' # put path of test dataset\n",
        "\n",
        "trainset = torchvision.datasets.ImageFolder(root= train_data_dir, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=4)\n",
        "\n",
        "valset = torchvision.datasets.ImageFolder(root= val_data_dir, transform=transform)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=4)\n",
        "\n",
        "testset = torchvision.datasets.ImageFolder(root= test_data_dir, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMLVEOfXeMkg"
      },
      "source": [
        "########################################################################\n",
        "# Define a Convolution Neural Network\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "# Copy the neural network from the Neural Networks section before and modify it to\n",
        "# take 3-channel images (instead of 1-channel images as it was defined).\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiH8ZT6FePqs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "fe97455c-f543-4388-b663-0f661e0ec5dd"
      },
      "source": [
        "# <<<<<<<<<<<<<<<<<<<<< EDIT THE MODEL DEFINITION >>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "# Try experimenting by changing the following:\n",
        "# 1. number of feature maps in conv layer\n",
        "# 2. Number of conv layers\n",
        "# 3. Kernel size\n",
        "# etc etc.,\n",
        "   \n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=128, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=5)\n",
        "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=5)\n",
        "        self.fc3 = nn.Linear(in_features=512, out_features=33)      # change out_features according to number of classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        global layer1\n",
        "        layer1 = x\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        global layer2\n",
        "        layer2 = x\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        global layer3\n",
        "        layer3 = x\n",
        "        x = F.avg_pool2d(x, kernel_size=x.shape[2:])\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  net = net.cuda()\n",
        "\n",
        "net.load_state_dict(torch.load('/content/drive/My Drive/Datasets/Final_Model.pth', map_location = torch.device('cpu')))\n",
        "net.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(3, 128, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv3): Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (fc3): Linear(in_features=512, out_features=33, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDqm0PoCl5oN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4b6bd85a-3c75-4196-9541-fca6b883fbf5"
      },
      "source": [
        "type(net.conv1.weight)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.nn.parameter.Parameter"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SwV6s76EQyR"
      },
      "source": [
        "########################################################################\n",
        "# Let us look at how the network performs on the test dataset.\n",
        "\n",
        "def test(testloader, model):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(testloader):\n",
        "            images, labels = data\n",
        "            if torch.cuda.is_available():\n",
        "                images, labels = images.cuda(), labels.cuda()        \n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the test images: %d %%' % (\n",
        "                                    100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2RX3xloCl0O"
      },
      "source": [
        "test(testloader, net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMLOiDizD-ic"
      },
      "source": [
        "def plot_filters_single_channel_big(t):\n",
        "    \n",
        "    #setting the rows and columns\n",
        "    nrows = t.shape[0]*t.shape[2]\n",
        "    ncols = t.shape[1]*t.shape[3]\n",
        "    \n",
        "    \n",
        "    npimg = np.array(t.numpy(), np.float32)\n",
        "    npimg = npimg.transpose((0, 2, 1, 3))\n",
        "    npimg = npimg.ravel().reshape(nrows, ncols)\n",
        "    \n",
        "    npimg = npimg.T\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(ncols/10, nrows/200))    \n",
        "    imgplot = sns.heatmap(npimg, xticklabels=False, yticklabels=False, cmap='gray', ax=ax, cbar=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdlGMDhZEdlC"
      },
      "source": [
        "def plot_filters_single_channel(t):\n",
        "    \n",
        "    #kernels depth * number of kernels\n",
        "    nplots = t.shape[0]*t.shape[1]\n",
        "    ncols = 12\n",
        "    \n",
        "    nrows = 1 + nplots//ncols\n",
        "    #convert tensor to numpy image\n",
        "    npimg = np.array(t.numpy(), np.float32)\n",
        "    \n",
        "    count = 0\n",
        "    fig = plt.figure(figsize=(ncols, nrows))\n",
        "    \n",
        "    #looping through all the kernels in each channel\n",
        "    for i in range(t.shape[0]):\n",
        "        for j in range(t.shape[1]):\n",
        "            count += 1\n",
        "            ax1 = fig.add_subplot(nrows, ncols, count)\n",
        "            npimg = np.array(t[i, j].numpy(), np.float32)\n",
        "            npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
        "            npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
        "            ax1.imshow(npimg)\n",
        "            ax1.set_title(str(i) + ',' + str(j))\n",
        "            ax1.axis('off')\n",
        "            ax1.set_xticklabels([])\n",
        "            ax1.set_yticklabels([])\n",
        "   \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vecwVP65EetV"
      },
      "source": [
        "def plot_filters_multi_channel(t):\n",
        "    \n",
        "    #get the number of kernals\n",
        "    num_kernels = t.shape[0]    \n",
        "    \n",
        "    #define number of columns for subplots\n",
        "    num_cols = 12\n",
        "    #rows = num of kernels\n",
        "    num_rows = num_kernels\n",
        "    \n",
        "    #set the figure size\n",
        "    fig = plt.figure(figsize=(num_cols,num_rows))\n",
        "    \n",
        "    #looping through all the kernels\n",
        "    for i in range(t.shape[0]):\n",
        "        ax1 = fig.add_subplot(num_rows,num_cols,i+1)\n",
        "        \n",
        "        #for each kernel, we convert the tensor to numpy \n",
        "        npimg = np.array(t[i].numpy(), np.float32)\n",
        "        #standardize the numpy image\n",
        "        npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
        "        npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
        "        npimg = npimg.transpose((1, 2, 0))\n",
        "        ax1.imshow(npimg)\n",
        "        ax1.axis('off')\n",
        "        ax1.set_title(str(i))\n",
        "        ax1.set_xticklabels([])\n",
        "        ax1.set_yticklabels([])\n",
        "        \n",
        "    plt.savefig('myimage.png', dpi=100)    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhL-uDSHF-j9"
      },
      "source": [
        "c1_tensor = net.conv1.weight.cpu().data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrZ2bvfeGQlu"
      },
      "source": [
        "plot_filters_multi_channel(c1_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImKJ2lvte-QT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "adaef2fc-35e0-4a82-a368-b32da4a76f73"
      },
      "source": [
        "img = Image.open(\"/content/drive/My Drive/Datasets/3/test/frying_pan/n0340023100000844.jpg\")\n",
        "print(type(img))\n",
        "trans1 = transforms.ToTensor()\n",
        "tensor = trans1(img)\n",
        "input = (tensor.unsqueeze(0))\n",
        "output = net(input)  \n",
        "print(torch.max(output[0]))\n",
        "print(output[0])\n",
        "print(torch.eq(torch.max(output[0]), output[0][10]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
            "tensor(5.6470, grad_fn=<MaxBackward1>)\n",
            "tensor([ 1.7186,  4.3766, -4.6335,  1.9029, -0.4405,  3.4578,  1.3772,  0.2593,\n",
            "         2.4365, -1.5706,  5.6470,  0.9799, -3.1920, -1.5036, -2.8335,  3.1338,\n",
            "        -0.3243, -3.4037,  3.1597,  5.2929,  3.1672, -3.0863, -0.9179, -1.6296,\n",
            "         0.0882, -1.1678, -3.3321, -1.9664, -5.2096, -1.8843,  0.7884, -0.1084,\n",
            "         0.0615], grad_fn=<SelectBackward>)\n",
            "tensor(True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeZMU-5xNuBg"
      },
      "source": [
        "net(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe4-B0LKQkb8"
      },
      "source": [
        "global layer1\n",
        "global layer2\n",
        "global layer3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsEg0FeNQL4E"
      },
      "source": [
        "torch.max(layer1[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahXNYZm_bAQA"
      },
      "source": [
        "list1 = [0,0,0,0,0]\n",
        "info_list1 =[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGOdlFHwTKGS"
      },
      "source": [
        "image_set = \"/content/drive/My Drive/Datasets/3/test/\"\n",
        "k=0\n",
        "min = 0\n",
        "for folder in os.listdir(image_set):\n",
        "  for filename in os.listdir(image_set + folder):\n",
        "    img_path = image_set+folder+'/'+filename\n",
        "    #print(image_set+folder+'/'+filename)\n",
        "    img = Image.open(img_path)\n",
        "    trans = transforms.ToTensor()\n",
        "    tensor = trans(img)\n",
        "    input = tensor.unsqueeze(0)  #.cuda()\n",
        "    #print(\"Output here\")        #output = net(input)\n",
        "    for i in range(layer1[0][0].shape[0]):\n",
        "      for j in range(layer1[0][0].shape[1]):\n",
        "        if layer1[0][0][i][j] > min:\n",
        "          list1.append(layer1[0][0][i][j])\n",
        "          list1.sort()\n",
        "          for k in range(len(info_list1)):\n",
        "            if info_list1[k][1] == min:\n",
        "              info_list1.pop(k)\n",
        "              break \n",
        "          list1.pop(0)\n",
        "          min = list1[0]\n",
        "          info_list1.append([img_path, layer1[0][0][i][j], i, j])\n",
        "\n",
        "        \n",
        "\n",
        "print(k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_drUtEQXqYW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "bb4686ef-e1dd-49a7-d7f6-23b6dd6a7c74"
      },
      "source": [
        "thing = [1,2,3]\n",
        "thing.sort()\n",
        "thing.pop(0)\n",
        "print(thing)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkWRoHf7hBt1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "38f08ce6-78da-44db-a543-0a3698bc0931"
      },
      "source": [
        "thing.append(\"ahdh\")\n",
        "thing.append(4)\n",
        "print(thing)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ahdh', 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVuJJxoshGwj"
      },
      "source": [
        "columns = ['Image Path', 'Response', 'i', 'j']\n",
        "df1 = pd.DataFrame(columns=columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQdk3orp31HZ"
      },
      "source": [
        "df1.loc[df1.shape[0]] = ['img2', 4,0,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEoqo-LA32Ov",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "75e1a714-3661-4a3c-d788-86f24373241c"
      },
      "source": [
        "df1.idxmax()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-feefca4c7926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36midxmax\u001b[0;34m(self, axis, skipna)\u001b[0m\n\u001b[1;32m   8865\u001b[0m         \"\"\"\n\u001b[1;32m   8866\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8867\u001b[0;31m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnanops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanargmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8869\u001b[0m         \u001b[0;31m# indices will always be np.ndarray since axis is not None and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mf_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nan\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 raise TypeError(\n\u001b[0;32m---> 67\u001b[0;31m                     \u001b[0;34mf\"reduction operation '{f_name}' not allowed for this dtype\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 )\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: reduction operation 'argmax' not allowed for this dtype"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-jmZkUY5L-W"
      },
      "source": [
        "responses = df1.iloc[:, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVMxG9M57eZY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "652abba8-b8ab-48af-b86a-af642cea0290"
      },
      "source": [
        "responses.idxmax()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOw5BriX77ai"
      },
      "source": [
        "df1[['Response', 'i', 'j']] = df1[['Response', 'i', 'j']].apply(pd.to_numeric) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6v22cAP9qMO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "3b92531a-f565-448e-ae0b-465346a8af40"
      },
      "source": [
        "df1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image Path</th>\n",
              "      <th>Response</th>\n",
              "      <th>i</th>\n",
              "      <th>j</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>img</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>img</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>img2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Image Path  Response  i  j\n",
              "0        img         0  0  0\n",
              "1        img         3  0  0\n",
              "2       img2         4  0  0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OmQDg0P9wok"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}