{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Samarth Ramesh NLP2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrptjV2IcSwp"
      },
      "source": [
        "# NLP Assignment 2\r\n",
        "Samarth Ramesh\r\n",
        "\r\n",
        "BMC201722"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1JhfK8v6ywx"
      },
      "source": [
        "import csv\r\n",
        "import collections\r\n",
        "from nltk.util import ngrams\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import random\r\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_FNi12DUaCm"
      },
      "source": [
        "#Loads the data from csv file at the end of the first assignment\r\n",
        "with open(\"/content/drive/MyDrive/Colab Data/words.csv\", 'r') as read_obj:\r\n",
        "    # pass the file object to reader() to get the reader object\r\n",
        "    csv_reader = csv.reader(read_obj)\r\n",
        "    # Pass reader object to list() to get a list of lists\r\n",
        "    words_data = list(csv_reader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVyzKqd2is41",
        "outputId": "5c3c57d6-e758-4ac7-d807-5a967a446f62"
      },
      "source": [
        "#Corpus size in number of words and senteneces and the vocabulary size.\r\n",
        "print(\"Number of sentences in the corpus: \"+str(len(words_data)))\r\n",
        "words = [word for sen in words_data for word in sen]\r\n",
        "print(\"Number of words in the corpus: \"+str(len(words)))\r\n",
        "print(\"Size of the vocabulary: \"+str(len(list(set(words)))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences in the corpus: 71703\n",
            "Number of words in the corpus: 1284387\n",
            "Size of the vocabulary: 58699\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJyvwuSqCeYr"
      },
      "source": [
        "def find_ngrams(words_data, n=3):\r\n",
        "  '''\r\n",
        "  Construct ngrams for some n value using words_data\r\n",
        "  '''\r\n",
        "  data = []\r\n",
        "  for sen in words_data:\r\n",
        "    sen_ngrams = list(ngrams(sen, n, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\r\n",
        "    data += sen_ngrams\r\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgZGkJzb9EOW"
      },
      "source": [
        "def build_trigram_model(trigrams_data, withcounts = False):\r\n",
        "  '''\r\n",
        "  Builds trigram model using trigrams generated from data. Counts occure of each trigram\r\n",
        "  and gives the likelihood of each trigram.\r\n",
        "  '''\r\n",
        "  trigram_model_counts = collections.defaultdict(lambda : collections.defaultdict(lambda : collections.defaultdict(lambda : 0)))\r\n",
        "  for gram in trigrams_data:\r\n",
        "    trigram_model_counts[gram[0]][gram[1]][gram[2]] +=1\r\n",
        "  trigram_model = trigram_model_counts\r\n",
        "  for w1 in trigram_model:\r\n",
        "    for w2 in trigram_model[w1]:\r\n",
        "      total = float(sum(trigram_model[w1][w2].values()))\r\n",
        "      for w3 in trigram_model[w1][w2]:\r\n",
        "        trigram_model[w1][w2][w3] /= total\r\n",
        "  if withcounts:\r\n",
        "    return trigram_model, trigram_model_counts\r\n",
        "  else:\r\n",
        "    return trigram_model\r\n",
        "\r\n",
        "def predict_next_word_tri(trigram_model, second_word='<s>', first_word='<s>', shouldplot = False):\r\n",
        "  '''\r\n",
        "  Predicts the next word using probabilities of trigram model and given firat and second word.\r\n",
        "  '''\r\n",
        "  third_word = trigram_model[first_word][second_word]\r\n",
        "  top10words = collections.Counter(third_word).most_common(10)\r\n",
        "\r\n",
        "  predicted_words = list(zip(*top10words))[0]\r\n",
        "  prob_scores = list(zip(*top10words))[1]\r\n",
        "  x_pos = np.arange(len(predicted_words))\r\n",
        "\r\n",
        "  if shouldplot:\r\n",
        "    plt.bar(x_pos, prob_scores, align = 'center')\r\n",
        "    plt.xticks(x_pos, predicted_words)\r\n",
        "    plt.ylabel('Probability Score')\r\n",
        "    plt.xlabel('Predicted Words')\r\n",
        "    plt.title('Predicted words for ' + first_word + ' ' + second_word)\r\n",
        "    plt.show()\r\n",
        "  return predicted_words[0]\r\n",
        "\r\n",
        "def generate_sentence_tri(predict_fn, trigram_model):\r\n",
        "  '''\r\n",
        "  Generates a sentence using the start token '<s>' as the first two words.\r\n",
        "  '''\r\n",
        "  first_word = predict_fn(trigram_model)\r\n",
        "  sentence = [first_word, predict_fn(trigram_model, first_word)]  \r\n",
        "  while 1>0:\r\n",
        "    next_word = predict_fn(trigram_model, sentence[-1], sentence[-2])\r\n",
        "    sentence.append(next_word)\r\n",
        "    if next_word == '</s>':\r\n",
        "      break\r\n",
        "  return \" \".join(sentence[:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "MuhPrxaJAuUN",
        "outputId": "457eb2bc-8a01-4b1e-d94e-84aacae1f10d"
      },
      "source": [
        "#Generating the trigram data, building the trigram model and generating a sentence.\r\n",
        "\r\n",
        "trigrams_data = find_ngrams(words_data, 3)\r\n",
        "trigram_model = build_trigram_model(trigrams_data)\r\n",
        "print(\"Sentence generated by the trigram model for the most common first word:\")\r\n",
        "generate_sentence_tri(predict_next_word_tri, trigram_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence generated by the trigram model for the most common first word:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the copyright holder for this preprint this version posted june 9 2020'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwQZoLseNr6V"
      },
      "source": [
        "def build_fgram_model(fgrams_data, withcounts = False):\r\n",
        "  '''\r\n",
        "  Build trigram model using trigrams generated from data. Counts occure of each trigram\r\n",
        "  and gives the likelihood of each trigram.\r\n",
        "  '''\r\n",
        "  fgram_model_counts = collections.defaultdict(lambda : collections.defaultdict(lambda : collections.defaultdict(lambda : collections.defaultdict(lambda : 0))))\r\n",
        "  for gram in fgrams_data:\r\n",
        "    fgram_model_counts[gram[0]][gram[1]][gram[2]][gram[3]] +=1\r\n",
        "  fgram_model = fgram_model_counts\r\n",
        "  for w1 in fgram_model:\r\n",
        "    for w2 in fgram_model[w1]:\r\n",
        "      for w3 in fgram_model[w1][w2]:\r\n",
        "        total = float(sum(fgram_model[w1][w2][w3].values()))\r\n",
        "        for w4 in fgram_model[w1][w2][w3]:\r\n",
        "          fgram_model[w1][w2][w3][w4] /= total\r\n",
        "  if withcounts:\r\n",
        "    return fgram_model, fgram_model_counts\r\n",
        "  else:\r\n",
        "    return fgram_model\r\n",
        "\r\n",
        "def predict_next_word_f(fgram_model, third_word = '<s>', second_word='<s>', first_word='<s>', shouldplot = False):\r\n",
        "  '''\r\n",
        "  Predicts the next word using probabilities of trigram model and given firat and second word.\r\n",
        "  '''\r\n",
        "  fourth_word = fgram_model[first_word][second_word][third_word]\r\n",
        "  top10words = collections.Counter(fourth_word).most_common(10)\r\n",
        "\r\n",
        "  predicted_words = list(zip(*top10words))[0]\r\n",
        "  prob_scores = list(zip(*top10words))[1]\r\n",
        "  x_pos = np.arange(len(predicted_words))\r\n",
        "\r\n",
        "  if shouldplot:\r\n",
        "    plt.bar(x_pos, prob_scores, align = 'center')\r\n",
        "    plt.xticks(x_pos, predicted_words)\r\n",
        "    plt.ylabel('Probability Score')\r\n",
        "    plt.xlabel('Predicted Words')\r\n",
        "    plt.title('Predicted words for ' + first_word + ' ' + second_word)\r\n",
        "    plt.show()\r\n",
        "  return predicted_words[0]\r\n",
        "\r\n",
        "def generate_sentence_f(predict_fn, fgram_model):\r\n",
        "  '''\r\n",
        "  Generates a sentence using the start token '<s>' as the first two words.\r\n",
        "  '''\r\n",
        "  first_word = predict_fn(fgram_model)\r\n",
        "  second_word = predict_fn(fgram_model, first_word)\r\n",
        "  sentence = [first_word, second_word, predict_fn(fgram_model, second_word, first_word)]  \r\n",
        "  while 1>0:\r\n",
        "    next_word = predict_fn(fgram_model, sentence[-1], sentence[-2], sentence[-3])\r\n",
        "    sentence.append(next_word)\r\n",
        "    if next_word == '</s>':\r\n",
        "      break\r\n",
        "  return \" \".join(sentence[:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "FuVUH9z2QXhO",
        "outputId": "243fbbc6-b5fb-4951-f1e6-eb052c95532f"
      },
      "source": [
        "#Generating the 4-gram data, building the 4-gram model and generating a sentence.\r\n",
        "\r\n",
        "fgrams_data = find_ngrams(words_data, 4)\r\n",
        "fgram_model = build_fgram_model(fgrams_data)\r\n",
        "print(\"Sentence generated by 4-gram model for the most common first word:\")\r\n",
        "generate_sentence_f(predict_next_word_f, fgram_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence generated by 4-gram model for the most common first word:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the copyright holder for this preprint this version posted june 9 2020'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obNrhkB5Q54D",
        "outputId": "ad72c2f1-1ccb-4702-b13c-70426ad55d3a"
      },
      "source": [
        "#Splitting the trigrams into train and test sets.\r\n",
        "\r\n",
        "random.shuffle(trigrams_data)\r\n",
        "l = len(trigrams_data)\r\n",
        "tri_train = trigrams_data[:(divmod(l,9)[0])]\r\n",
        "tri_test = trigrams_data[(divmod(l,9)[0]):]\r\n",
        "tri_no_test = len(tri_test)\r\n",
        "\r\n",
        "#Adding the cross-entropy loss for each trigram in the test set.\r\n",
        "\r\n",
        "tri_model = build_trigram_model(tri_train)\r\n",
        "loss = 0\r\n",
        "for trigram in tri_test:\r\n",
        "  prob_correct = max(tri_model[trigram[0]][trigram[1]][trigram[2]], 0.000001)\r\n",
        "  loss += (-1)*math.log10(prob_correct)\r\n",
        "print(\"Average Cross Entropy Loss of the trigram model is \"+str(loss/tri_no_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Cross Entropy Loss of the trigram model is 4.709567274267211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRbDtDkKSkL5",
        "outputId": "403909fc-ff21-470d-f005-168aba30c2a9"
      },
      "source": [
        "#Splitting the 4-grams into train and test sets.\r\n",
        "\r\n",
        "random.shuffle(fgrams_data)\r\n",
        "l = len(fgrams_data)\r\n",
        "f_train = fgrams_data[:(divmod(l,9)[0])]\r\n",
        "f_test = fgrams_data[(divmod(l,9)[0]):]\r\n",
        "f_no_test = len(f_test)\r\n",
        "\r\n",
        "#Adding the cross-entropy loss for each 4-gram in the test set.\r\n",
        "\r\n",
        "f_model = build_fgram_model(f_train)\r\n",
        "loss = 0\r\n",
        "for fgram in f_test:\r\n",
        "  prob_correct = max(f_model[fgram[0]][fgram[1]][fgram[2]][fgram[3]], 0.000001)\r\n",
        "  loss += (-1)*math.log10(prob_correct)\r\n",
        "print(\"Average Cross Entropy Loss of the 4-gram model is \"+str(loss/f_no_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Cross Entropy Loss of the 4-gram model is 5.070950454264896\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLzqhqHkboCW"
      },
      "source": [
        "Note: The largest possible cross entropy loss for ngrams that exist in the data is -log(1/number of ngrams). Hence as long as the loss for the zero occurence case is set to higher than that, the loss is valid. So the loss for zero occurence is set to 6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVXwX2wUdKJL"
      },
      "source": [
        "## Trigram Language Model Vs 4-gram Language Model\r\n",
        "\r\n",
        "The observed average cross-entropy loss of each ngram in the test set is lower for the trigram model. This goes against our expectations because one would think a more robust model like the 4-gram would do better. This may come down to a lack of data because 350 papers doesn't do a good job representing all possible ngrams of words.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p52pMO9ahgMJ"
      },
      "source": [
        "## Handling Large Set of Parameters\r\n",
        "\r\n",
        "If stored naively a trigram model would be of order (vocabulary)^3. Therefore storing only the necessary trigrams is very important since the counts in model are very sparse. A default dictionary is very helpful in this situation because initialising the counts is not necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG2whP0af-k9"
      },
      "source": [
        "## Executing sections of the code parallelly\r\n",
        "\r\n",
        "The trigram and the 4-gram portions of the code can be executed seperately. Actually, if written differently all steps of each model building process can be performed simultaneously. Each sentence can be broken down into ngrams and used to build the model parallely. "
      ]
    }
  ]
}